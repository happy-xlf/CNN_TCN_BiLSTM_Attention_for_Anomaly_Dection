{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27c7e94",
   "metadata": {
    "id": "b27c7e94"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import  torch.optim as optim\n",
    "from    matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.rc('font',family='Times New Roman', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d42449",
   "metadata": {
    "id": "50d42449"
   },
   "outputs": [],
   "source": [
    "def convert_2d(df_dup):\n",
    "    data_frame = pd.DataFrame()\n",
    "    for i in range(0, df_dup.shape[0]-59):\n",
    "        is_anomaly = False\n",
    "        mylist = []\n",
    "        for j in range(i, i+60):\n",
    "            mylist.append(df_dup['value'].iat[j])\n",
    "            if df_dup['is_anomaly'].iat[j] == 1:\n",
    "                is_anomaly = True\n",
    "        if is_anomaly:\n",
    "            mylist.append(1)\n",
    "        else:\n",
    "            mylist.append(0)\n",
    "        np_Array = np.array(mylist)\n",
    "        mylist = np_Array.T\n",
    "        data_frame = data_frame.append(pd.Series(mylist), ignore_index=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41caeb84",
   "metadata": {
    "id": "41caeb84"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dataset_conc=[]\n",
    "    path=r'Dataset'\n",
    "    all_files=glob.glob(path+\"/*.csv\")\n",
    "    for filename in all_files:\n",
    "        df=pd.read_csv(filename,index_col=None,header=0)\n",
    "        #将数据中value为0的替换成NaN\n",
    "        df=df.replace(0,np.nan)\n",
    "        #处理value那层数据，将0去除掉\n",
    "        df=df.dropna(axis=0, how='any',subset=['value'])\n",
    "        df.value = preprocessing.normalize([df.value]).T\n",
    "        dataset_conc.append(convert_2d(df))\n",
    "    frame=pd.concat(dataset_conc,axis=0,ignore_index=True)\n",
    "    y = frame.iloc[:, 60]\n",
    "    X = frame.iloc[:, 0:60]\n",
    "    X_train = X[:int(X.shape[0] * 0.7)]\n",
    "    X_test = X[int(X.shape[0] * 0.7):]\n",
    "    y_train = y[:int(X.shape[0] * 0.7)]\n",
    "    y_test = y[int(X.shape[0] * 0.7):]\n",
    "\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    nrows, ncols = X_train.shape\n",
    "    X_train = X_train.reshape(nrows, ncols, 1)\n",
    "\n",
    "    X_test = X_test.to_numpy()\n",
    "    nrows, ncols = X_test.shape\n",
    "    X_test = X_test.reshape(nrows, ncols, 1)\n",
    "\n",
    "    y_test = y_test.to_numpy()\n",
    "    # print(\"X_train:\",X_train.shape)\n",
    "    #[62107,60,1]\n",
    "    # print(\"y_train:\",y_train.shape)\n",
    "    #[62107,]\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a511c60a",
   "metadata": {
    "id": "a511c60a"
   },
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mu7NvVCtmFvx",
   "metadata": {
    "id": "mu7NvVCtmFvx"
   },
   "outputs": [],
   "source": [
    "# 定义实现因果卷积的类\n",
    "from torch.nn.utils import weight_norm\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "# 定义了一个残差模块\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # n_inputs:输入通道数\n",
    "        # n_outputs:输出通道数\n",
    "        # stride：步长\n",
    "        # padding:填充长度\n",
    "        # dilation：扩张率\n",
    "        # 定义第一个空洞卷积层\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        # 根据第一个卷积层的输出与padding大小实现因果卷积\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        # 添加激活函数与dropout正则化方法完成第一个卷积\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 堆叠同样结构的第二个卷积层\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 将卷积模块的所有组建通过Sequential方法依次堆叠在一起\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "\n",
    "        # 如果输出纬度和输入维度不一致，则必须对输出进行1X1卷积\n",
    "        # 如果通道数不一样，那么需要对输入x做一个逐元素的一维卷积以使得它的纬度与前面两个卷积相等。\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        # 不同激活函数的尝试\n",
    "        # self.sigmod = nn.Softmax()\n",
    "        # self.tanh = nn.Tanh()\n",
    "        # self.softPlus = nn.Softplus()\n",
    "        # self.leaky = nn.LeakyReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    # 初始化为从均值为0，标准差为0.01的正态分布中采样的随机值\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    # 结合卷积与输入的恒等映射（或输入的逐元素卷积），并投入ReLU 激活函数完成残差模块\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "# 时间卷积网络\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.5):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        # num_input:输入特征数，默认为1\n",
    "        # num_levels:网络层数，每一层是一个残差块\n",
    "        # num_channels:储存了所有层级的输出通道数\n",
    "        layers = []\n",
    "        # num_channels为各层卷积运算的输出通道数或卷积核数量\n",
    "        num_levels = len(num_channels)\n",
    "        # 空洞卷积的扩张系数若随着网络层级的增加而成指数级增加，则可以增大感受野并不丢弃任何输入序列的元素\n",
    "        # dilation_size根据层级数成指数增加，并从num_channels中抽取每一个残差模块的输入通道数与输出通道数\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        # 将所有残差模块堆叠起来组成一个深度卷积网络\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.network(x)\n",
    "        #print('tcn_shape:',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cf220",
   "metadata": {
    "id": "449cf220"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            #[512,1,60]\n",
    "            nn.Conv1d(in_channels=1,out_channels=64,kernel_size=3,stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,stride=2),\n",
    "            #[512,32,30]\n",
    "            nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            #[512,64,15]\n",
    "        )\n",
    "        self.lstm=nn.LSTM(input_size=960, hidden_size=64, num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            #[512,128]\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2),\n",
    "            #[512,2]\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.conv(x)\n",
    "        out=out.reshape(-1,1,15*64)\n",
    "        #[512,1,960]\n",
    "        out, _ = self.lstm(out)\n",
    "        #[512,1,128]\n",
    "        out=self.fc(out)\n",
    "        #[512,2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9LFcbLdh3P00",
   "metadata": {
    "id": "9LFcbLdh3P00"
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            #[512,1,60]\n",
    "            nn.Conv1d(in_channels=1,out_channels=64,kernel_size=3,stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2,stride=2),\n",
    "            #[512,32,30]\n",
    "            nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,stride=1,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            #[512,64,15]\n",
    "        )\n",
    "        self.lstm=nn.GRU(input_size=960, hidden_size=64, num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            #[512,128]\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,2),\n",
    "            #[512,2]\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.conv(x)\n",
    "        out=out.reshape(-1,1,15*64)\n",
    "        #[512,1,960]\n",
    "        out, _ = self.lstm(out)\n",
    "        #[512,1,128]\n",
    "        out=self.fc(out)\n",
    "        #[512,2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IY48TYAmKq1m",
   "metadata": {
    "id": "IY48TYAmKq1m"
   },
   "outputs": [],
   "source": [
    "loss_list=[]\n",
    "epoch_list=[]\n",
    "acc_list=[]\n",
    "ans_acc_list=[]\n",
    "train_acc_list=[]\n",
    "\n",
    "loss_list2=[]\n",
    "epoch_list2=[]\n",
    "acc_list2=[]\n",
    "ans_acc_list2=[]\n",
    "train_acc_list2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f0697",
   "metadata": {
    "id": "a28f0697"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from numpy import *\n",
    "def train(input,y_train,X_test,y_test,new_input,new_y_train):\n",
    "    torch_dataset=Data.TensorDataset(input,y_train)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        shuffle=True,  # 要不要打乱数据 (打乱比较好)\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "    # print(len(loader))\n",
    "    #122\n",
    "    net = Net()\n",
    "    net=net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(521):\n",
    "        # 在一轮中迭代获取每个batch（把全部的数据分成小块一块块的训练）\n",
    "        net.train()\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            input=batch_x.to(device)\n",
    "            label=batch_y.to(device)\n",
    "            # print(\"input:\",input.shape)\n",
    "            #[512,1,60]\n",
    "            # print(\"label:\",label.shape)\n",
    "            #[512,2]\n",
    "            y_pred=net(input)\n",
    "            loss = F.binary_cross_entropy(y_pred,label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             for p in net.parameters():\n",
    "#               # print(p.grad.norm())                 # 查看参数p的梯度\n",
    "#               torch.nn.utils.clip_grad_norm_(p, 10)  # 将梯度裁剪到小于10\n",
    "            optimizer.step()\n",
    "        flag=False\n",
    "        if epoch%30==0:\n",
    "          flag=True\n",
    "        acc=test(net,X_test,y_test,flag)\n",
    "        train_acc=test(net,new_input,new_y_train,flag)\n",
    "        print('epoch', epoch, ':', loss.item(),'train_acc',train_acc,'test_acc：',acc)\n",
    "        acc_list.append(acc)\n",
    "        ans_acc_list.append(acc)\n",
    "        train_acc_list.append(train_acc)\n",
    "        loss_list.append(loss.item())\n",
    "        epoch_list.append(epoch)\n",
    "        if epoch%100==0:\n",
    "            print(\"acc平均值：\",round(mean(acc_list),4))\n",
    "            acc_list.clear()\n",
    "    torch.save(net.state_dict(),'model/net_bilstm_params.pth')\n",
    "    plt.plot(epoch_list,loss_list,color='red',label='training Loss')\n",
    "    plt.plot(epoch_list,ans_acc_list,color='green',label='test Acc')\n",
    "    plt.plot(epoch_list,train_acc_list,color='blue',label='training Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yrekleFRkUiA",
   "metadata": {
    "id": "yrekleFRkUiA"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from numpy import *\n",
    "def train2(input,y_train,X_test,y_test,new_input,new_y_train):\n",
    "    torch_dataset=Data.TensorDataset(input,y_train)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        shuffle=True,  # 要不要打乱数据 (打乱比较好)\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "    # print(len(loader))\n",
    "    #122\n",
    "    net = Net2()\n",
    "    net=net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(521):\n",
    "        # 在一轮中迭代获取每个batch（把全部的数据分成小块一块块的训练）\n",
    "        net.train()\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            input=batch_x.to(device)\n",
    "            label=batch_y.to(device)\n",
    "            # print(\"input:\",input.shape)\n",
    "            #[512,1,60]\n",
    "            # print(\"label:\",label.shape)\n",
    "            #[512,2]\n",
    "            y_pred=net(input)\n",
    "            loss = F.binary_cross_entropy(y_pred,label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             for p in net.parameters():\n",
    "#               # print(p.grad.norm())                 # 查看参数p的梯度\n",
    "#               torch.nn.utils.clip_grad_norm_(p, 10)  # 将梯度裁剪到小于10\n",
    "            optimizer.step()\n",
    "        flag=False\n",
    "        if epoch%30==0:\n",
    "          flag=True\n",
    "        acc=test(net,X_test,y_test,flag)\n",
    "        train_acc=test(net,new_input,new_y_train,flag)\n",
    "        print('epoch', epoch, ':', loss.item(),'train_acc',train_acc,'test_acc：',acc)\n",
    "        acc_list2.append(acc)\n",
    "        ans_acc_list2.append(acc)\n",
    "        train_acc_list2.append(train_acc)\n",
    "        loss_list2.append(loss.item())\n",
    "        epoch_list2.append(epoch)\n",
    "        if epoch%100==0:\n",
    "            print(\"acc平均值：\",round(mean(acc_list2),4))\n",
    "            acc_list2.clear()\n",
    "    torch.save(net.state_dict(),'model/net_bigru_params.pth')\n",
    "    plt.plot(epoch_list2,loss_list2,color='red',label='training Loss')\n",
    "    plt.plot(epoch_list2,ans_acc_list2,color='green',label='test Acc')\n",
    "    plt.plot(epoch_list2,train_acc_list2,color='blue',label='training Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13c97c",
   "metadata": {
    "id": "cc13c97c"
   },
   "outputs": [],
   "source": [
    "def test(model,x_test,y_test,flag):\n",
    "    model.eval()\n",
    "    torch_dataset=Data.TensorDataset(x_test,y_test)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "\n",
    "    acc = 0.0\n",
    "    count = 0\n",
    "    ans_labels=[]\n",
    "    ans_pre=[]\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data  # 5,3,400,600  5,10\n",
    "        count += len(labels)\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        acc += (labels == predict).sum().item()\n",
    "        ans_labels+=labels.cpu().numpy().tolist()\n",
    "        ans_pre+=predict.cpu().numpy().tolist()\n",
    "    #evaluate performance\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    if flag==True:\n",
    "      Confusion_Matrix = confusion_matrix(ans_labels, ans_pre)\n",
    "      Accuracy = accuracy_score(ans_labels, ans_pre)\n",
    "      precision = precision_score(ans_labels, ans_pre, average='binary')\n",
    "      recall = recall_score(ans_labels, ans_pre, average='binary')\n",
    "      F1_Score = f1_score(ans_labels, ans_pre, average='binary')\n",
    "      print(\"Confusion_Matrix\")\n",
    "      print(Confusion_Matrix)\n",
    "      print(\"Accuracy \", Accuracy)\n",
    "      print(\"Precision \", precision)\n",
    "      print(\"recall \", recall)\n",
    "      print(\"f1_score \", F1_Score)\n",
    "    return round(acc/count,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc2e3c",
   "metadata": {
    "id": "81cc2e3c"
   },
   "outputs": [],
   "source": [
    "new_y_train=y_train\n",
    "new_y_train=torch.tensor(new_y_train)\n",
    "new_x_train=X_train\n",
    "new_input=torch.tensor(new_x_train).permute(0,2,1).to(torch.float32)\n",
    "y_train=F.one_hot(torch.tensor(y_train).to(torch.int64),2)\n",
    "y_train=y_train.to(torch.float32)\n",
    "#[batch_size,seq_len,embedding_size]=>[batch_size,embeding_size,seq_len]\n",
    "input=torch.tensor(X_train).permute(0,2,1).to(torch.float32)\n",
    "X_test = torch.tensor(X_test).permute(0, 2, 1).to(torch.float32)\n",
    "y_test=torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02149f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "02149f11",
    "outputId": "c118e145-e37b-49b8-ca0b-a0ffffb8187a"
   },
   "outputs": [],
   "source": [
    "model=train(input,y_train,X_test,y_test,new_input,new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9f4YhnskpAe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l9f4YhnskpAe",
    "outputId": "88ad569e-7483-45a2-89ac-17b2d0fb26f9"
   },
   "outputs": [],
   "source": [
    "model2=train2(input,y_train,X_test,y_test,new_input,new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9333f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "f7e9333f",
    "outputId": "d361805a-d70c-4aeb-c242-239ca2396354"
   },
   "outputs": [],
   "source": [
    "# plt.plot(epoch_list,loss_list,color='red',label='training Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.yticks([0.8,0.85,0.90,0.95,1])\n",
    "plt.plot(epoch_list,ans_acc_list,color='blue',label='CNN_BiLSTM')\n",
    "plt.plot(epoch_list2,ans_acc_list2,color='red',label='CNN_BiGRU')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ILPtw9pKcPk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "-ILPtw9pKcPk",
    "outputId": "220c647a-6bbf-4571-f7a0-71cd1be5b9ec"
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch_list,loss_list,color='blue',label='CNN_BiLSTM')\n",
    "plt.plot(epoch_list2,loss_list2,color='red',label='CNN_BiGRU')\n",
    "# plt.yticks([0.8,0.85,0.90,0.95,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jY6r-s_qaFUr",
   "metadata": {
    "id": "jY6r-s_qaFUr"
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "xl = xlwt.Workbook()\n",
    "# 调用对象的add_sheet方法\n",
    "sheet1 = xl.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "\n",
    "sheet1.write(0, 0, \"cnn_bilstm-accuracy\")\n",
    "sheet1.write(0, 1, 'cnn_bilstm-loss')\n",
    "\n",
    "\n",
    "for i in range(0, len(ans_acc_list)):\n",
    "    sheet1.write(i + 1, 0, ans_acc_list[i])\n",
    "    sheet1.write(i + 1, 1, loss_list[i])\n",
    "\n",
    "xl.save(\"csv/cnn_bilstm.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a59c94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "xl = xlwt.Workbook()\n",
    "# 调用对象的add_sheet方法\n",
    "sheet1 = xl.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "\n",
    "sheet1.write(0, 0, \"cnn_bigru-accuracy\")\n",
    "sheet1.write(0, 1, 'cnn_bigru-loss')\n",
    "\n",
    "\n",
    "for i in range(0, len(ans_acc_list2)):\n",
    "    sheet1.write(i + 1, 0, ans_acc_list2[i])\n",
    "    sheet1.write(i + 1, 1, loss_list2[i])\n",
    "\n",
    "xl.save(\"csv/cnn_bigru.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "模型整理.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
