{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27c7e94",
   "metadata": {
    "id": "b27c7e94"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import  torch.optim as optim\n",
    "from    matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.rc('font',family='Times New Roman', size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eVAMj3MaibNd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVAMj3MaibNd",
    "outputId": "9a950752-534d-4071-e897-a07758ed4a1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d42449",
   "metadata": {
    "id": "50d42449"
   },
   "outputs": [],
   "source": [
    "def convert_2d(df_dup):\n",
    "    data_frame = pd.DataFrame()\n",
    "    for i in range(0, df_dup.shape[0]-59):\n",
    "        is_anomaly = False\n",
    "        mylist = []\n",
    "        for j in range(i, i+60):\n",
    "            mylist.append(df_dup['value'].iat[j])\n",
    "            if df_dup['is_anomaly'].iat[j] == 1:\n",
    "                is_anomaly = True\n",
    "        if is_anomaly:\n",
    "            mylist.append(1)\n",
    "        else:\n",
    "            mylist.append(0)\n",
    "        np_Array = np.array(mylist)\n",
    "        mylist = np_Array.T\n",
    "        data_frame = data_frame.append(pd.Series(mylist), ignore_index=True)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41caeb84",
   "metadata": {
    "id": "41caeb84"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dataset_conc=[]\n",
    "    path=r'Dataset'\n",
    "    all_files=glob.glob(path+\"/*.csv\")\n",
    "    for filename in all_files:\n",
    "        df=pd.read_csv(filename,index_col=None,header=0)\n",
    "        #将数据中value为0的替换成NaN\n",
    "        df=df.replace(0,np.nan)\n",
    "        #处理value那层数据，将0去除掉\n",
    "        df=df.dropna(axis=0, how='any',subset=['value'])\n",
    "        df.value = preprocessing.normalize([df.value]).T\n",
    "        dataset_conc.append(convert_2d(df))\n",
    "    frame=pd.concat(dataset_conc,axis=0,ignore_index=True)\n",
    "    y = frame.iloc[:, 60]\n",
    "    X = frame.iloc[:, 0:60]\n",
    "    X_train = X[:int(X.shape[0] * 0.7)]\n",
    "    X_test = X[int(X.shape[0] * 0.7):]\n",
    "    y_train = y[:int(X.shape[0] * 0.7)]\n",
    "    y_test = y[int(X.shape[0] * 0.7):]\n",
    "\n",
    "\n",
    "    X_train = X_train.to_numpy()\n",
    "    nrows, ncols = X_train.shape\n",
    "    X_train = X_train.reshape(nrows, ncols, 1)\n",
    "\n",
    "    X_test = X_test.to_numpy()\n",
    "    nrows, ncols = X_test.shape\n",
    "    X_test = X_test.reshape(nrows, ncols, 1)\n",
    "\n",
    "    y_test = y_test.to_numpy()\n",
    "    # print(\"X_train:\",X_train.shape)\n",
    "    #[62107,60,1]\n",
    "    # print(\"y_train:\",y_train.shape)\n",
    "    #[62107,]\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a511c60a",
   "metadata": {
    "id": "a511c60a"
   },
   "outputs": [],
   "source": [
    "X_train,y_train,X_test,y_test=get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mu7NvVCtmFvx",
   "metadata": {
    "id": "mu7NvVCtmFvx"
   },
   "outputs": [],
   "source": [
    "# 定义实现因果卷积的类\n",
    "from torch.nn.utils import weight_norm\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "# 定义了一个残差模块\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        # n_inputs:输入通道数\n",
    "        # n_outputs:输出通道数\n",
    "        # stride：步长\n",
    "        # padding:填充长度\n",
    "        # dilation：扩张率\n",
    "        # 定义第一个空洞卷积层\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        # 根据第一个卷积层的输出与padding大小实现因果卷积\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        # 添加激活函数与dropout正则化方法完成第一个卷积\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 堆叠同样结构的第二个卷积层\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 将卷积模块的所有组建通过Sequential方法依次堆叠在一起\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "\n",
    "        # 如果输出纬度和输入维度不一致，则必须对输出进行1X1卷积\n",
    "        # 如果通道数不一样，那么需要对输入x做一个逐元素的一维卷积以使得它的纬度与前面两个卷积相等。\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        # 不同激活函数的尝试\n",
    "        # self.sigmod = nn.Softmax()\n",
    "        # self.tanh = nn.Tanh()\n",
    "        # self.softPlus = nn.Softplus()\n",
    "        # self.leaky = nn.LeakyReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    # 初始化为从均值为0，标准差为0.01的正态分布中采样的随机值\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    # 结合卷积与输入的恒等映射（或输入的逐元素卷积），并投入ReLU 激活函数完成残差模块\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "# 时间卷积网络\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=5, dropout=0.5):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        # num_input:输入特征数，默认为1\n",
    "        # num_levels:网络层数，每一层是一个残差块\n",
    "        # num_channels:储存了所有层级的输出通道数\n",
    "        layers = []\n",
    "        # num_channels为各层卷积运算的输出通道数或卷积核数量\n",
    "        num_levels = len(num_channels)\n",
    "        # 空洞卷积的扩张系数若随着网络层级的增加而成指数级增加，则可以增大感受野并不丢弃任何输入序列的元素\n",
    "        # dilation_size根据层级数成指数增加，并从num_channels中抽取每一个残差模块的输入通道数与输出通道数\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "        # 将所有残差模块堆叠起来组成一个深度卷积网络\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.network(x)\n",
    "        #print('tcn_shape:',x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "449cf220",
   "metadata": {
    "id": "449cf220"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            #[512,1,60]\n",
    "            TemporalConvNet(1,[64,64,64]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            #[512,64,30]\n",
    "        )\n",
    "        self.bilstm=nn.LSTM(input_size=1920, hidden_size=64, num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            #[512,128]\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32,2),\n",
    "            #[512,2]\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        batch_size = len(lstm_output)\n",
    "        hidden = final_state.view(batch_size, -1,\n",
    "                                  1)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "\n",
    "        # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.conv(x)\n",
    "        out=out.reshape(-1,1,30*64)\n",
    "        #[512,1,960]\n",
    "        out,(final_hidden_state,final_cell_state) = self.bilstm(out)\n",
    "        attn_out,attention=self.attention_net(out,final_hidden_state)\n",
    "        #[512,128]\n",
    "        out=self.fc(attn_out)\n",
    "        #[512,2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "WfWrC5Rckeay",
   "metadata": {
    "id": "WfWrC5Rckeay"
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            #[512,1,60]\n",
    "            TemporalConvNet(1,[64,64,64]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, stride=2),\n",
    "            #[512,64,30]\n",
    "        )\n",
    "        self.bigru=nn.GRU(input_size=1920, hidden_size=64, num_layers=1,batch_first=True,bidirectional=True)\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            #[512,128]\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(32,2),\n",
    "            #[512,2]\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        batch_size = len(lstm_output)\n",
    "        hidden = final_state.view(batch_size, -1,\n",
    "                                  1)  # hidden : [batch_size, n_hidden * num_directions(=2), n_layer(=1)]\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # attn_weights : [batch_size, n_step]\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "\n",
    "        # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return context, soft_attn_weights\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.conv(x)\n",
    "        out=out.reshape(-1,1,30*64)\n",
    "        #[512,1,960]\n",
    "        out,final_hidden_state=self.bigru(out)\n",
    "        attn_out,attention=self.attention_net(out,final_hidden_state)\n",
    "        #[512,128]\n",
    "        out=self.fc(attn_out)\n",
    "        #[512,2]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "IY48TYAmKq1m",
   "metadata": {
    "id": "IY48TYAmKq1m"
   },
   "outputs": [],
   "source": [
    "loss_list=[]\n",
    "epoch_list=[]\n",
    "acc_list=[]\n",
    "ans_acc_list=[]\n",
    "train_acc_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47exa8HJkcBW",
   "metadata": {
    "id": "47exa8HJkcBW"
   },
   "outputs": [],
   "source": [
    "loss_list2=[]\n",
    "epoch_list2=[]\n",
    "acc_list2=[]\n",
    "ans_acc_list2=[]\n",
    "train_acc_list2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a28f0697",
   "metadata": {
    "id": "a28f0697"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from numpy import *\n",
    "def train(input,y_train,X_test,y_test,new_input,new_y_train):\n",
    "    torch_dataset=Data.TensorDataset(input,y_train)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        shuffle=True,  # 要不要打乱数据 (打乱比较好)\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "    # print(len(loader))\n",
    "    #122\n",
    "    net = Net()\n",
    "    net=net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(501):\n",
    "        # 在一轮中迭代获取每个batch（把全部的数据分成小块一块块的训练）\n",
    "        net.train()\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            input=batch_x.to(device)\n",
    "            label=batch_y.to(device)\n",
    "            # print(\"input:\",input.shape)\n",
    "            #[512,1,60]\n",
    "            # print(\"label:\",label.shape)\n",
    "            #[512,2]\n",
    "            y_pred=net(input)\n",
    "            loss = F.binary_cross_entropy(y_pred,label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             for p in net.parameters():\n",
    "#               # print(p.grad.norm())                 # 查看参数p的梯度\n",
    "#               torch.nn.utils.clip_grad_norm_(p, 10)  # 将梯度裁剪到小于10\n",
    "            optimizer.step()\n",
    "        flag=False\n",
    "        if epoch%30==0:\n",
    "          flag=True\n",
    "        acc=test(net,X_test,y_test,flag)\n",
    "        train_acc=test(net,new_input,new_y_train,flag)\n",
    "        print('epoch', epoch, ':', loss.item(),'train_acc',train_acc,'test_acc：',acc)\n",
    "        acc_list.append(acc)\n",
    "        ans_acc_list.append(acc)\n",
    "        train_acc_list.append(train_acc)\n",
    "        loss_list.append(loss.item())\n",
    "        epoch_list.append(epoch)\n",
    "        if epoch%100==0:\n",
    "            print(\"acc平均值：\",round(mean(acc_list),4))\n",
    "            acc_list.clear()\n",
    "    torch.save(net.state_dict(),'model/net_tcn_bilstm_attention_params.pth')\n",
    "    plt.plot(epoch_list,loss_list,color='red',label='training Loss')\n",
    "    plt.plot(epoch_list,ans_acc_list,color='green',label='test Acc')\n",
    "    plt.plot(epoch_list,train_acc_list,color='blue',label='training Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iWt7V9dekL3y",
   "metadata": {
    "id": "iWt7V9dekL3y"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "from numpy import *\n",
    "def train2(input,y_train,X_test,y_test,new_input,new_y_train):\n",
    "    torch_dataset=Data.TensorDataset(input,y_train)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        shuffle=True,  # 要不要打乱数据 (打乱比较好)\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "    # print(len(loader))\n",
    "    #122\n",
    "    net = Net2()\n",
    "    net=net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "    net.train()\n",
    "\n",
    "    for epoch in range(501):\n",
    "        # 在一轮中迭代获取每个batch（把全部的数据分成小块一块块的训练）\n",
    "        net.train()\n",
    "        for step, (batch_x, batch_y) in enumerate(loader):\n",
    "            input=batch_x.to(device)\n",
    "            label=batch_y.to(device)\n",
    "            # print(\"input:\",input.shape)\n",
    "            #[512,1,60]\n",
    "            # print(\"label:\",label.shape)\n",
    "            #[512,2]\n",
    "            y_pred=net(input)\n",
    "            loss = F.binary_cross_entropy(y_pred,label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             for p in net.parameters():\n",
    "#               # print(p.grad.norm())                 # 查看参数p的梯度\n",
    "#               torch.nn.utils.clip_grad_norm_(p, 10)  # 将梯度裁剪到小于10\n",
    "            optimizer.step()\n",
    "        flag=False\n",
    "        if epoch%30==0:\n",
    "          flag=True\n",
    "        acc=test(net,X_test,y_test,flag)\n",
    "        train_acc=test(net,new_input,new_y_train,flag)\n",
    "        print('epoch', epoch, ':', loss.item(),'train_acc',train_acc,'test_acc：',acc)\n",
    "        acc_list2.append(acc)\n",
    "        ans_acc_list2.append(acc)\n",
    "        train_acc_list2.append(train_acc)\n",
    "        loss_list2.append(loss.item())\n",
    "        epoch_list2.append(epoch)\n",
    "        if epoch%100==0:\n",
    "            print(\"acc平均值：\",round(mean(acc_list2),4))\n",
    "            acc_list2.clear()\n",
    "    torch.save(net.state_dict(),'model/net_tcn_bigru_attention_params.pth')\n",
    "    plt.plot(epoch_list2,loss_list2,color='red',label='training Loss')\n",
    "    plt.plot(epoch_list2,ans_acc_list2,color='green',label='test Acc')\n",
    "    plt.plot(epoch_list2,train_acc_list2,color='blue',label='training Acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc13c97c",
   "metadata": {
    "id": "cc13c97c"
   },
   "outputs": [],
   "source": [
    "def test(model,x_test,y_test,flag):\n",
    "    model.eval()\n",
    "    torch_dataset=Data.TensorDataset(x_test,y_test)\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,  # 数据，封装进Data.TensorDataset()类的数据\n",
    "        batch_size=512,  # 每块的大小\n",
    "        num_workers=0,  # 多进程（multiprocess）来读数据\n",
    "    )\n",
    "\n",
    "    acc = 0.0\n",
    "    count = 0\n",
    "    ans_labels=[]\n",
    "    ans_pre=[]\n",
    "    for index, data in enumerate(loader):\n",
    "        inputs, labels = data  # 5,3,400,600  5,10\n",
    "        count += len(labels)\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        acc += (labels == predict).sum().item()\n",
    "        ans_labels+=labels.cpu().numpy().tolist()\n",
    "        ans_pre+=predict.cpu().numpy().tolist()\n",
    "    #evaluate performance\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    if flag==True:\n",
    "      Confusion_Matrix = confusion_matrix(ans_labels, ans_pre)\n",
    "      Accuracy = accuracy_score(ans_labels, ans_pre)\n",
    "      precision = precision_score(ans_labels, ans_pre, average='binary')\n",
    "      recall = recall_score(ans_labels, ans_pre, average='binary')\n",
    "      F1_Score = f1_score(ans_labels, ans_pre, average='binary')\n",
    "      print(\"Confusion_Matrix\")\n",
    "      print(Confusion_Matrix)\n",
    "      print(\"Accuracy \", Accuracy)\n",
    "      print(\"Precision \", precision)\n",
    "      print(\"recall \", recall)\n",
    "      print(\"f1_score \", F1_Score)\n",
    "    return round(acc/count,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81cc2e3c",
   "metadata": {
    "id": "81cc2e3c"
   },
   "outputs": [],
   "source": [
    "new_y_train=y_train\n",
    "new_y_train=torch.tensor(new_y_train)\n",
    "new_x_train=X_train\n",
    "new_input=torch.tensor(new_x_train).permute(0,2,1).to(torch.float32)\n",
    "y_train=F.one_hot(torch.tensor(y_train).to(torch.int64),2)\n",
    "y_train=y_train.to(torch.float32)\n",
    "#[batch_size,seq_len,embedding_size]=>[batch_size,embeding_size,seq_len]\n",
    "input=torch.tensor(X_train).permute(0,2,1).to(torch.float32)\n",
    "X_test = torch.tensor(X_test).permute(0, 2, 1).to(torch.float32)\n",
    "y_test=torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02149f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "02149f11",
    "outputId": "538115e4-7098-4c7a-a6ba-689cc9fbe79f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code_software\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix\n",
      "[[23986     0]\n",
      " [ 2632     0]]\n",
      "Accuracy  0.9011195431662784\n",
      "Precision  0.0\n",
      "recall  0.0\n",
      "f1_score  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\code_software\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion_Matrix\n",
      "[[56266     0]\n",
      " [ 5841     0]]\n",
      "Accuracy  0.9059526301383097\n",
      "Precision  0.0\n",
      "recall  0.0\n",
      "f1_score  0.0\n",
      "epoch 0 : 0.34341442584991455 train_acc 0.906 test_acc： 0.9011\n",
      "acc平均值： 0.9011\n",
      "epoch 1 : 0.4362630844116211 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 2 : 0.32311591506004333 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 3 : 0.35827675461769104 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 4 : 0.31125348806381226 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 5 : 0.31644004583358765 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 6 : 0.3651573956012726 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 7 : 0.24640701711177826 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 8 : 0.3640218675136566 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 9 : 0.2561621367931366 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 10 : 0.33239760994911194 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 11 : 0.2785983085632324 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 12 : 0.33210405707359314 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 13 : 0.2693953514099121 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 14 : 0.2860702872276306 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 15 : 0.2543925642967224 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 16 : 0.37952107191085815 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 17 : 0.2975144684314728 train_acc 0.906 test_acc： 0.9011\n",
      "epoch 18 : 0.2682627737522125 train_acc 0.9099 test_acc： 0.9033\n",
      "epoch 19 : 0.32152435183525085 train_acc 0.9111 test_acc： 0.9052\n",
      "epoch 20 : 0.39379262924194336 train_acc 0.9124 test_acc： 0.9052\n",
      "epoch 21 : 0.25050652027130127 train_acc 0.913 test_acc： 0.9079\n",
      "epoch 22 : 0.35520538687705994 train_acc 0.9136 test_acc： 0.9096\n",
      "epoch 23 : 0.28500035405158997 train_acc 0.9161 test_acc： 0.91\n",
      "epoch 24 : 0.18598274886608124 train_acc 0.9149 test_acc： 0.9121\n",
      "epoch 25 : 0.18876676261425018 train_acc 0.9143 test_acc： 0.9135\n",
      "epoch 26 : 0.3326539397239685 train_acc 0.915 test_acc： 0.9143\n",
      "epoch 27 : 0.307542622089386 train_acc 0.9127 test_acc： 0.9132\n",
      "epoch 28 : 0.3575197458267212 train_acc 0.9127 test_acc： 0.9135\n",
      "epoch 29 : 0.14838965237140656 train_acc 0.9128 test_acc： 0.9139\n",
      "Confusion_Matrix\n",
      "[[23986     0]\n",
      " [ 2283   349]]\n",
      "Accuracy  0.9142309715230296\n",
      "Precision  1.0\n",
      "recall  0.13259878419452886\n",
      "f1_score  0.23414961422341493\n",
      "Confusion_Matrix\n",
      "[[56198    68]\n",
      " [ 5365   476]]\n",
      "Accuracy  0.9125219379458032\n",
      "Precision  0.875\n",
      "recall  0.08149289505221709\n",
      "f1_score  0.14909945184025059\n",
      "epoch 30 : 0.2943492531776428 train_acc 0.9125 test_acc： 0.9142\n",
      "epoch 31 : 0.3445170819759369 train_acc 0.9129 test_acc： 0.9145\n",
      "epoch 32 : 0.3341808021068573 train_acc 0.9132 test_acc： 0.9148\n",
      "epoch 33 : 0.3064488172531128 train_acc 0.9134 test_acc： 0.9154\n",
      "epoch 34 : 0.31128430366516113 train_acc 0.9122 test_acc： 0.9145\n",
      "epoch 35 : 0.2996709942817688 train_acc 0.9126 test_acc： 0.9146\n",
      "epoch 36 : 0.24164895713329315 train_acc 0.9133 test_acc： 0.9155\n",
      "epoch 37 : 0.3283248841762543 train_acc 0.9132 test_acc： 0.9153\n",
      "epoch 38 : 0.30973342061042786 train_acc 0.9128 test_acc： 0.9154\n",
      "epoch 39 : 0.3133791983127594 train_acc 0.9134 test_acc： 0.9157\n",
      "epoch 40 : 0.30640923976898193 train_acc 0.9129 test_acc： 0.9155\n",
      "epoch 41 : 0.24402844905853271 train_acc 0.9125 test_acc： 0.9152\n",
      "epoch 42 : 0.25535309314727783 train_acc 0.9129 test_acc： 0.9156\n",
      "epoch 43 : 0.2684156596660614 train_acc 0.9138 test_acc： 0.9164\n",
      "epoch 44 : 0.3374130427837372 train_acc 0.913 test_acc： 0.9159\n",
      "epoch 45 : 0.22422927618026733 train_acc 0.9127 test_acc： 0.9157\n",
      "epoch 46 : 0.2761733829975128 train_acc 0.9136 test_acc： 0.9163\n",
      "epoch 47 : 0.37567201256752014 train_acc 0.9132 test_acc： 0.916\n",
      "epoch 48 : 0.1276472508907318 train_acc 0.9136 test_acc： 0.9163\n",
      "epoch 49 : 0.2543083429336548 train_acc 0.9144 test_acc： 0.9168\n",
      "epoch 50 : 0.15272411704063416 train_acc 0.9137 test_acc： 0.9163\n",
      "epoch 51 : 0.24264097213745117 train_acc 0.9138 test_acc： 0.9167\n",
      "epoch 52 : 0.2575092911720276 train_acc 0.9136 test_acc： 0.916\n",
      "epoch 53 : 0.31939488649368286 train_acc 0.9133 test_acc： 0.9164\n",
      "epoch 54 : 0.307327538728714 train_acc 0.9138 test_acc： 0.9169\n",
      "epoch 55 : 0.3288930654525757 train_acc 0.914 test_acc： 0.9174\n",
      "epoch 56 : 0.17014822363853455 train_acc 0.9142 test_acc： 0.9175\n",
      "epoch 57 : 0.22850199043750763 train_acc 0.9143 test_acc： 0.9174\n",
      "epoch 58 : 0.2889907658100128 train_acc 0.9146 test_acc： 0.9176\n",
      "epoch 59 : 0.2865184247493744 train_acc 0.9163 test_acc： 0.9177\n",
      "Confusion_Matrix\n",
      "[[23986     0]\n",
      " [ 2203   429]]\n",
      "Accuracy  0.9172364565331731\n",
      "Precision  1.0\n",
      "recall  0.16299392097264437\n",
      "f1_score  0.2803005553740608\n",
      "Confusion_Matrix\n",
      "[[56207    59]\n",
      " [ 5290   551]]\n",
      "Accuracy  0.9138744424944049\n",
      "Precision  0.9032786885245901\n",
      "recall  0.0943331621297723\n",
      "f1_score  0.17082622849170673\n",
      "epoch 60 : 0.27327510714530945 train_acc 0.9139 test_acc： 0.9172\n",
      "epoch 61 : 0.39737096428871155 train_acc 0.9159 test_acc： 0.9178\n",
      "epoch 62 : 0.23045602440834045 train_acc 0.9163 test_acc： 0.918\n",
      "epoch 63 : 0.1965862363576889 train_acc 0.9152 test_acc： 0.9177\n",
      "epoch 64 : 0.3017474114894867 train_acc 0.9168 test_acc： 0.918\n",
      "epoch 65 : 0.2541108727455139 train_acc 0.9161 test_acc： 0.9179\n",
      "epoch 66 : 0.25938308238983154 train_acc 0.9161 test_acc： 0.918\n",
      "epoch 67 : 0.15474554896354675 train_acc 0.9179 test_acc： 0.9185\n",
      "epoch 68 : 0.2768820822238922 train_acc 0.9181 test_acc： 0.9187\n",
      "epoch 69 : 0.1910589635372162 train_acc 0.9181 test_acc： 0.9193\n",
      "epoch 70 : 0.22413748502731323 train_acc 0.918 test_acc： 0.9187\n",
      "epoch 71 : 0.258100301027298 train_acc 0.916 test_acc： 0.918\n",
      "epoch 72 : 0.23817044496536255 train_acc 0.9178 test_acc： 0.9186\n",
      "epoch 73 : 0.22281523048877716 train_acc 0.9172 test_acc： 0.9185\n",
      "epoch 74 : 0.16499774158000946 train_acc 0.9172 test_acc： 0.9184\n",
      "epoch 75 : 0.1822369396686554 train_acc 0.9185 test_acc： 0.9189\n",
      "epoch 76 : 0.19561634957790375 train_acc 0.9192 test_acc： 0.9194\n",
      "epoch 77 : 0.1749790459871292 train_acc 0.9184 test_acc： 0.9187\n",
      "epoch 78 : 0.32865744829177856 train_acc 0.9192 test_acc： 0.9189\n",
      "epoch 79 : 0.3163667917251587 train_acc 0.9189 test_acc： 0.919\n",
      "epoch 80 : 0.2226661890745163 train_acc 0.9189 test_acc： 0.9186\n",
      "epoch 81 : 0.2833118140697479 train_acc 0.9193 test_acc： 0.9192\n",
      "epoch 82 : 0.23673607409000397 train_acc 0.9174 test_acc： 0.9182\n",
      "epoch 83 : 0.322642058134079 train_acc 0.9183 test_acc： 0.9186\n",
      "epoch 84 : 0.23406982421875 train_acc 0.919 test_acc： 0.9195\n",
      "epoch 85 : 0.21454890072345734 train_acc 0.919 test_acc： 0.919\n",
      "epoch 86 : 0.2983899414539337 train_acc 0.9175 test_acc： 0.9182\n",
      "epoch 87 : 0.2560736835002899 train_acc 0.9178 test_acc： 0.9181\n",
      "epoch 88 : 0.29918888211250305 train_acc 0.9184 test_acc： 0.9183\n",
      "epoch 89 : 0.25975337624549866 train_acc 0.9176 test_acc： 0.9182\n",
      "Confusion_Matrix\n",
      "[[23986     0]\n",
      " [ 2160   472]]\n",
      "Accuracy  0.9188519047261252\n",
      "Precision  1.0\n",
      "recall  0.17933130699088146\n",
      "f1_score  0.30412371134020616\n",
      "Confusion_Matrix\n",
      "[[56193    73]\n",
      " [ 4934   907]]\n",
      "Accuracy  0.9193810681565685\n",
      "Precision  0.9255102040816326\n",
      "recall  0.15528162985790103\n",
      "f1_score  0.2659434100571763\n",
      "epoch 90 : 0.24147407710552216 train_acc 0.9194 test_acc： 0.9189\n",
      "epoch 91 : 0.33858224749565125 train_acc 0.92 test_acc： 0.9199\n",
      "epoch 92 : 0.2814696133136749 train_acc 0.9191 test_acc： 0.9188\n",
      "epoch 93 : 0.2222486436367035 train_acc 0.9195 test_acc： 0.9187\n",
      "epoch 94 : 0.20856623351573944 train_acc 0.9205 test_acc： 0.9201\n",
      "epoch 95 : 0.12975510954856873 train_acc 0.9194 test_acc： 0.9189\n",
      "epoch 96 : 0.19645632803440094 train_acc 0.9198 test_acc： 0.9189\n",
      "epoch 97 : 0.266983300447464 train_acc 0.9199 test_acc： 0.9195\n",
      "epoch 98 : 0.16711702942848206 train_acc 0.9199 test_acc： 0.919\n",
      "epoch 99 : 0.27894267439842224 train_acc 0.9199 test_acc： 0.9194\n",
      "epoch 100 : 0.25458163022994995 train_acc 0.9214 test_acc： 0.921\n",
      "acc平均值： 0.9139\n",
      "epoch 101 : 0.10349521040916443 train_acc 0.9189 test_acc： 0.9188\n",
      "epoch 102 : 0.14673659205436707 train_acc 0.9194 test_acc： 0.919\n",
      "epoch 103 : 0.2730102241039276 train_acc 0.9234 test_acc： 0.9204\n",
      "epoch 104 : 0.27081120014190674 train_acc 0.9215 test_acc： 0.92\n",
      "epoch 105 : 0.19139693677425385 train_acc 0.9198 test_acc： 0.919\n",
      "epoch 106 : 0.19785629212856293 train_acc 0.9216 test_acc： 0.9198\n",
      "epoch 107 : 0.23972384631633759 train_acc 0.9197 test_acc： 0.9189\n",
      "epoch 108 : 0.19303372502326965 train_acc 0.9214 test_acc： 0.9198\n",
      "epoch 109 : 0.23432336747646332 train_acc 0.9215 test_acc： 0.9193\n",
      "epoch 110 : 0.25869348645210266 train_acc 0.9218 test_acc： 0.9199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 111 : 0.18167462944984436 train_acc 0.9205 test_acc： 0.9194\n",
      "epoch 112 : 0.2653345763683319 train_acc 0.9233 test_acc： 0.9202\n",
      "epoch 113 : 0.22673410177230835 train_acc 0.9204 test_acc： 0.919\n",
      "epoch 114 : 0.20315468311309814 train_acc 0.9235 test_acc： 0.9196\n",
      "epoch 115 : 0.25399234890937805 train_acc 0.9219 test_acc： 0.9201\n",
      "epoch 116 : 0.2680138349533081 train_acc 0.924 test_acc： 0.9204\n",
      "epoch 117 : 0.1883796900510788 train_acc 0.9214 test_acc： 0.9197\n",
      "epoch 118 : 0.2668007016181946 train_acc 0.9225 test_acc： 0.9201\n",
      "epoch 119 : 0.1570121943950653 train_acc 0.9215 test_acc： 0.9195\n",
      "Confusion_Matrix\n",
      "[[23983     3]\n",
      " [ 2129   503]]\n",
      "Accuracy  0.9199038244796754\n",
      "Precision  0.9940711462450593\n",
      "recall  0.1911094224924012\n",
      "f1_score  0.3205863607393244\n",
      "Confusion_Matrix\n",
      "[[56262     4]\n",
      " [ 4806  1035]]\n",
      "Accuracy  0.9225530133479318\n",
      "Precision  0.9961501443695862\n",
      "recall  0.17719568567026195\n",
      "f1_score  0.30087209302325585\n",
      "epoch 120 : 0.2189677506685257 train_acc 0.9226 test_acc： 0.9199\n",
      "epoch 121 : 0.41281628608703613 train_acc 0.923 test_acc： 0.9196\n",
      "epoch 122 : 0.1618960052728653 train_acc 0.9204 test_acc： 0.9193\n",
      "epoch 123 : 0.1916092187166214 train_acc 0.9209 test_acc： 0.9192\n",
      "epoch 124 : 0.32493939995765686 train_acc 0.921 test_acc： 0.9193\n",
      "epoch 125 : 0.19321133196353912 train_acc 0.9215 test_acc： 0.919\n",
      "epoch 126 : 0.23187388479709625 train_acc 0.9211 test_acc： 0.919\n",
      "epoch 127 : 0.16720089316368103 train_acc 0.9227 test_acc： 0.9191\n",
      "epoch 128 : 0.25969988107681274 train_acc 0.9224 test_acc： 0.9193\n",
      "epoch 129 : 0.34134379029273987 train_acc 0.9228 test_acc： 0.9195\n",
      "epoch 130 : 0.15902328491210938 train_acc 0.9224 test_acc： 0.9194\n",
      "epoch 131 : 0.24591419100761414 train_acc 0.9232 test_acc： 0.9197\n",
      "epoch 132 : 0.1876305788755417 train_acc 0.9231 test_acc： 0.9197\n",
      "epoch 133 : 0.23268505930900574 train_acc 0.924 test_acc： 0.9194\n",
      "epoch 134 : 0.21212969720363617 train_acc 0.9214 test_acc： 0.919\n",
      "epoch 135 : 0.22761136293411255 train_acc 0.9222 test_acc： 0.919\n",
      "epoch 136 : 0.31240513920783997 train_acc 0.9227 test_acc： 0.9195\n",
      "epoch 137 : 0.2022976577281952 train_acc 0.9226 test_acc： 0.9193\n",
      "epoch 138 : 0.2786633372306824 train_acc 0.9218 test_acc： 0.919\n",
      "epoch 139 : 0.2223115712404251 train_acc 0.9236 test_acc： 0.9197\n",
      "epoch 140 : 0.27198880910873413 train_acc 0.9246 test_acc： 0.9196\n",
      "epoch 141 : 0.23024941980838776 train_acc 0.9234 test_acc： 0.9189\n",
      "epoch 142 : 0.2148512303829193 train_acc 0.9219 test_acc： 0.9187\n",
      "epoch 143 : 0.21799181401729584 train_acc 0.925 test_acc： 0.9201\n",
      "epoch 144 : 0.23942169547080994 train_acc 0.9241 test_acc： 0.9192\n",
      "epoch 145 : 0.10922814160585403 train_acc 0.9241 test_acc： 0.9191\n",
      "epoch 146 : 0.19300396740436554 train_acc 0.9233 test_acc： 0.9195\n",
      "epoch 147 : 0.132697194814682 train_acc 0.9238 test_acc： 0.9197\n",
      "epoch 148 : 0.20092464983463287 train_acc 0.9243 test_acc： 0.9198\n",
      "epoch 149 : 0.09041404724121094 train_acc 0.9229 test_acc： 0.9192\n",
      "Confusion_Matrix\n",
      "[[23980     6]\n",
      " [ 2135   497]]\n",
      "Accuracy  0.9195657074160343\n",
      "Precision  0.9880715705765407\n",
      "recall  0.18882978723404256\n",
      "f1_score  0.3170653907496013\n",
      "Confusion_Matrix\n",
      "[[56258     8]\n",
      " [ 4719  1122]]\n",
      "Accuracy  0.9238894166519072\n",
      "Precision  0.9929203539823008\n",
      "recall  0.192090395480226\n",
      "f1_score  0.3219050351456032\n",
      "epoch 150 : 0.21806396543979645 train_acc 0.9239 test_acc： 0.9196\n",
      "epoch 151 : 0.19248759746551514 train_acc 0.9226 test_acc： 0.9198\n",
      "epoch 152 : 0.25755777955055237 train_acc 0.9247 test_acc： 0.9199\n",
      "epoch 153 : 0.1888611763715744 train_acc 0.9228 test_acc： 0.9195\n",
      "epoch 154 : 0.20619753003120422 train_acc 0.9223 test_acc： 0.9192\n",
      "epoch 155 : 0.2778705060482025 train_acc 0.9228 test_acc： 0.9192\n",
      "epoch 156 : 0.2118990272283554 train_acc 0.9251 test_acc： 0.9196\n",
      "epoch 157 : 0.12729355692863464 train_acc 0.924 test_acc： 0.9192\n",
      "epoch 158 : 0.27453887462615967 train_acc 0.925 test_acc： 0.9198\n",
      "epoch 159 : 0.14277517795562744 train_acc 0.926 test_acc： 0.9199\n",
      "epoch 160 : 0.2599819600582123 train_acc 0.9227 test_acc： 0.9194\n",
      "epoch 161 : 0.22982056438922882 train_acc 0.925 test_acc： 0.9204\n",
      "epoch 162 : 0.18460194766521454 train_acc 0.9239 test_acc： 0.9199\n",
      "epoch 163 : 0.25101014971733093 train_acc 0.9274 test_acc： 0.9202\n",
      "epoch 164 : 0.20128904283046722 train_acc 0.9263 test_acc： 0.9203\n",
      "epoch 165 : 0.2613511085510254 train_acc 0.9248 test_acc： 0.9198\n",
      "epoch 166 : 0.2542708218097687 train_acc 0.9247 test_acc： 0.9198\n",
      "epoch 167 : 0.18168693780899048 train_acc 0.9234 test_acc： 0.9199\n",
      "epoch 168 : 0.22242426872253418 train_acc 0.9196 test_acc： 0.9184\n",
      "epoch 169 : 0.24289953708648682 train_acc 0.9181 test_acc： 0.9185\n",
      "epoch 170 : 0.16750679910182953 train_acc 0.9182 test_acc： 0.9189\n",
      "epoch 171 : 0.2644660472869873 train_acc 0.9185 test_acc： 0.9185\n",
      "epoch 172 : 0.2420247197151184 train_acc 0.919 test_acc： 0.9182\n",
      "epoch 173 : 0.24897955358028412 train_acc 0.9196 test_acc： 0.9181\n",
      "epoch 174 : 0.269440233707428 train_acc 0.9201 test_acc： 0.9183\n",
      "epoch 175 : 0.14861765503883362 train_acc 0.9208 test_acc： 0.9186\n",
      "epoch 176 : 0.14324267208576202 train_acc 0.9207 test_acc： 0.9187\n",
      "epoch 177 : 0.2539653778076172 train_acc 0.9194 test_acc： 0.9185\n",
      "epoch 178 : 0.199547678232193 train_acc 0.9192 test_acc： 0.9182\n",
      "epoch 179 : 0.235954150557518 train_acc 0.9213 test_acc： 0.9189\n",
      "Confusion_Matrix\n",
      "[[23982     4]\n",
      " [ 2158   474]]\n",
      "Accuracy  0.9187767676008716\n",
      "Precision  0.9916317991631799\n",
      "recall  0.18009118541033434\n",
      "f1_score  0.3048231511254019\n",
      "Confusion_Matrix\n",
      "[[56266     0]\n",
      " [ 4937   904]]\n",
      "Accuracy  0.9205081552804032\n",
      "Precision  1.0\n",
      "recall  0.15476801917479885\n",
      "f1_score  0.2680504077094144\n",
      "epoch 180 : 0.2021169662475586 train_acc 0.9205 test_acc： 0.9188\n",
      "epoch 181 : 0.2218163013458252 train_acc 0.9208 test_acc： 0.9187\n",
      "epoch 182 : 0.2198234498500824 train_acc 0.9207 test_acc： 0.9184\n",
      "epoch 183 : 0.1787809282541275 train_acc 0.9201 test_acc： 0.9183\n",
      "epoch 184 : 0.2599826455116272 train_acc 0.9207 test_acc： 0.9183\n",
      "epoch 185 : 0.25745466351509094 train_acc 0.9207 test_acc： 0.9188\n",
      "epoch 186 : 0.13801686465740204 train_acc 0.9209 test_acc： 0.9184\n",
      "epoch 187 : 0.23990197479724884 train_acc 0.9225 test_acc： 0.919\n",
      "epoch 188 : 0.2116585522890091 train_acc 0.9226 test_acc： 0.9193\n",
      "epoch 189 : 0.26759853959083557 train_acc 0.9216 test_acc： 0.9187\n",
      "epoch 190 : 0.21891124546527863 train_acc 0.9222 test_acc： 0.9193\n",
      "epoch 191 : 0.259906530380249 train_acc 0.9212 test_acc： 0.9192\n",
      "epoch 192 : 0.24583952128887177 train_acc 0.9224 test_acc： 0.9194\n",
      "epoch 193 : 0.2555152177810669 train_acc 0.9229 test_acc： 0.9197\n",
      "epoch 194 : 0.27873295545578003 train_acc 0.9233 test_acc： 0.9198\n",
      "epoch 195 : 0.22688131034374237 train_acc 0.9227 test_acc： 0.9196\n",
      "epoch 196 : 0.22425363957881927 train_acc 0.9226 test_acc： 0.9198\n",
      "epoch 197 : 0.15877273678779602 train_acc 0.9233 test_acc： 0.9198\n",
      "epoch 198 : 0.21191538870334625 train_acc 0.922 test_acc： 0.9201\n",
      "epoch 199 : 0.23679012060165405 train_acc 0.924 test_acc： 0.9201\n",
      "epoch 200 : 0.18851493299007416 train_acc 0.9249 test_acc： 0.9201\n",
      "acc平均值： 0.9193\n",
      "epoch 201 : 0.18408513069152832 train_acc 0.9241 test_acc： 0.9203\n",
      "epoch 202 : 0.1336217075586319 train_acc 0.9238 test_acc： 0.9202\n",
      "epoch 203 : 0.2974507808685303 train_acc 0.9238 test_acc： 0.9205\n",
      "epoch 204 : 0.2414427250623703 train_acc 0.9235 test_acc： 0.9202\n",
      "epoch 205 : 0.19045671820640564 train_acc 0.9249 test_acc： 0.9201\n",
      "epoch 206 : 0.247737318277359 train_acc 0.9259 test_acc： 0.9205\n",
      "epoch 207 : 0.2062668800354004 train_acc 0.925 test_acc： 0.921\n",
      "epoch 208 : 0.2197536826133728 train_acc 0.9249 test_acc： 0.9207\n",
      "epoch 209 : 0.2184792459011078 train_acc 0.9247 test_acc： 0.9203\n",
      "Confusion_Matrix\n",
      "[[23974    12]\n",
      " [ 2102   530]]\n",
      "Accuracy  0.9205800586069577\n",
      "Precision  0.977859778597786\n",
      "recall  0.2013677811550152\n",
      "f1_score  0.33396345305608066\n",
      "Confusion_Matrix\n",
      "[[56262     4]\n",
      " [ 4694  1147]]\n",
      "Accuracy  0.9243563527460673\n",
      "Precision  0.996524761077324\n",
      "recall  0.19637048450607772\n",
      "f1_score  0.3280892448512586\n",
      "epoch 210 : 0.15427637100219727 train_acc 0.9244 test_acc： 0.9206\n",
      "epoch 211 : 0.12508176267147064 train_acc 0.9256 test_acc： 0.9209\n",
      "epoch 212 : 0.18735331296920776 train_acc 0.9237 test_acc： 0.9204\n",
      "epoch 213 : 0.15059925615787506 train_acc 0.9252 test_acc： 0.9208\n",
      "epoch 214 : 0.15138134360313416 train_acc 0.9261 test_acc： 0.9211\n",
      "epoch 215 : 0.23574252426624298 train_acc 0.9267 test_acc： 0.921\n",
      "epoch 216 : 0.20432049036026 train_acc 0.9246 test_acc： 0.9207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 217 : 0.13394127786159515 train_acc 0.926 test_acc： 0.9208\n",
      "epoch 218 : 0.25294965505599976 train_acc 0.9268 test_acc： 0.9208\n",
      "epoch 219 : 0.2826460301876068 train_acc 0.9255 test_acc： 0.9206\n",
      "epoch 220 : 0.1481134444475174 train_acc 0.9267 test_acc： 0.9211\n",
      "epoch 221 : 0.2501256465911865 train_acc 0.9259 test_acc： 0.9211\n",
      "epoch 222 : 0.1269046813249588 train_acc 0.9255 test_acc： 0.9211\n",
      "epoch 223 : 0.26995447278022766 train_acc 0.9262 test_acc： 0.9207\n",
      "epoch 224 : 0.2374974638223648 train_acc 0.9261 test_acc： 0.9209\n",
      "epoch 225 : 0.16246142983436584 train_acc 0.9262 test_acc： 0.9213\n",
      "epoch 226 : 0.15252356231212616 train_acc 0.927 test_acc： 0.9215\n"
     ]
    }
   ],
   "source": [
    "model=train(input,y_train,X_test,y_test,new_input,new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SQn545_XlY-f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQn545_XlY-f",
    "outputId": "7e32e99f-b092-4dc4-a4c4-ad47ee0739bd"
   },
   "outputs": [],
   "source": [
    "model2=train2(input,y_train,X_test,y_test,new_input,new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7e9333f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f7e9333f"
   },
   "outputs": [],
   "source": [
    "# plt.plot(epoch_list,loss_list,color='red',label='training Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(epoch_list,ans_acc_list,color='red',label='CNN_TCN_BiLSTM_Attention')\n",
    "# plt.plot(epoch_list,ans_acc_list2,color='blue',label='CNN_TCN_BiGRU_Attention')\n",
    "plt.yticks([0.8,0.85,0.90,0.95,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ILPtw9pKcPk",
   "metadata": {
    "id": "-ILPtw9pKcPk"
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(epoch_list,loss_list,color='red',label='CNN_TCN_BiLSTM_Attention')\n",
    "plt.plot(epoch_list,loss_list2,color='blue',label='CNN_TCN_BiGRU_Attention')\n",
    "# plt.yticks([0.8,0.85,0.90,0.95,1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "jY6r-s_qaFUr",
   "metadata": {
    "id": "jY6r-s_qaFUr"
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "xl = xlwt.Workbook()\n",
    "# 调用对象的add_sheet方法\n",
    "sheet1 = xl.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "\n",
    "sheet1.write(0, 0, \"cnn_tcn_bilstm_attention_accuracy\")\n",
    "sheet1.write(0, 1, 'cnn_tcn_bilstm_attention_loss')\n",
    "\n",
    "\n",
    "for i in range(0, len(ans_acc_list)):\n",
    "    sheet1.write(i + 1, 0, ans_acc_list[i])\n",
    "    sheet1.write(i + 1, 1, loss_list[i])\n",
    "\n",
    "xl.save(\"csv/cnn_tcn_bilstm_attention.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HnLDkBdamXy1",
   "metadata": {
    "id": "HnLDkBdamXy1"
   },
   "outputs": [],
   "source": [
    "import xlwt\n",
    "xl = xlwt.Workbook()\n",
    "# 调用对象的add_sheet方法\n",
    "sheet1 = xl.add_sheet('sheet1', cell_overwrite_ok=True)\n",
    "\n",
    "sheet1.write(0, 0, \"cnn_tcn_bigru_attention_accuracy\")\n",
    "sheet1.write(0, 1, 'cnn_tcn_bigru_attention_loss')\n",
    "\n",
    "\n",
    "for i in range(0, len(ans_acc_list2)):\n",
    "    sheet1.write(i + 1, 0, ans_acc_list2[i])\n",
    "    sheet1.write(i + 1, 1, loss_list2[i])\n",
    "\n",
    "xl.save(\"csv/cnn_tcn_bigru_attention.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_TCN_BiGRU_Attention与CNN_TCN_BiLSTM_Attention图表制作.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
